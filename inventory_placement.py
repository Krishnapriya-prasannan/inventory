"""
Inventory Placement Optimization Module

Greedy heuristic to assign high-demand items to most accessible warehouse locations.

INPUT FILES (expected in current working directory):
  1. item_attributes.csv
      Required columns:
        - item_id (string/int)
        - demand_frequency (numeric, higher = more demand)
        - dimensions (string, e.g. "10x20x30" or "10*20*30" or "10 X 20 X 30") -> height x width x depth (units consistent with shelf size)
        - current_stock (int)
        Optional columns:
        - weight_per_unit (numeric, defaults to 1.0 if missing)
  2. warehouse_layout.csv
      Required columns:
        - location_id (string)
        - x_coord (numeric)
        - y_coord (numeric)
        - max_size (numeric)  (capacity in volume units)
        - max_weight (numeric) (capacity in weight units)
        - (optional) location_type (string; rows containing 'packing' (case-insensitive) will be excluded)
        - (optional) is_packing_station (bool/int; if True/1 excludes)

PACKING STATION:
  - Assumed at coordinate (0, 0) unless explicitly marked via a location_type containing 'packing'.
  - Distances for ranking shelves calculated via Euclidean distance to (0,0).

OUTPUT:
  - placement_recommendations.csv with columns:
        item_id, recommended_location
    If an item cannot be placed under constraints, recommended_location will be 'UNPLACED'.

USAGE:
  Basic:
      python inventory_placement.py
  Optional arguments:
      --items path/to/item_attributes.csv
      --layout path/to/warehouse_layout.csv
      --output path/to/placement_recommendations.csv
      --verbose (enable detailed logging)

ASSUMPTIONS & NOTES:
  - Greedy algorithm: items sorted by demand_frequency descending; locations sorted by distance ascending.
  - Each location holds AT MOST one item (as requested by description). Extend logic if multi-item storage is needed.
  - Volume = height * width * depth parsed from dimensions string; parsing is forgiving (extracts the first 3 numbers).
  - Weight per unit defaults to 1.0 if missing.
  - current_stock * weight_per_unit must fit within max_weight.
  - Items with malformed dimensions or missing capacity data will be marked UNPLACED with a warning.

EXTENSIONS (future ideas):
  - Permit partial fills or multiple items per shelf with residual capacity tracking.
  - Add tie-breakers (e.g., ABC classification clusters, adjacency constraints).
  - Integrate travel time / aisle structure rather than pure Euclidean distance.

Author: Auto-generated by GitHub Copilot
"""
from __future__ import annotations

import argparse
import logging
import math
import os
import re
import sys
from dataclasses import dataclass
from typing import List, Optional

import pandas as pd

LOGGER = logging.getLogger("inventory_placement")

DIMENSION_REGEX = re.compile(r"(\d+(?:\.\d+)?)")  # captures numbers (ints or floats)


@dataclass
class ItemRecord:
    item_id: str
    demand_frequency: float
    height: float
    width: float
    depth: float
    volume: float
    current_stock: int
    weight_per_unit: float

    @property
    def total_weight(self) -> float:
        return self.current_stock * self.weight_per_unit


@dataclass
class LocationRecord:
    location_id: str
    x: float
    y: float
    max_size: float
    max_weight: float
    distance: float
    occupied: bool = False


def parse_dimensions(dimensions: str) -> Optional[List[float]]:
    if not isinstance(dimensions, str):
        return None
    nums = DIMENSION_REGEX.findall(dimensions.replace(",", " "))
    if len(nums) < 3:
        return None
    try:
        h, w, d = map(float, nums[:3])
        return [h, w, d]
    except ValueError:
        return None


def load_items(path: str) -> List[ItemRecord]:
    df = pd.read_csv(path)
    required = {"item_id", "demand_frequency", "dimensions", "current_stock"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Missing required item columns: {missing}")

    if "weight_per_unit" not in df.columns:
        df["weight_per_unit"] = 1.0

    items: List[ItemRecord] = []
    for idx, row in df.iterrows():
        dims = parse_dimensions(row["dimensions"])
        if not dims:
            LOGGER.warning("Item %s has invalid dimensions '%s' -> will be UNPLACED later", row["item_id"], row["dimensions"])
            # store with zero volume to trigger fail
            dims = [0.0, 0.0, 0.0]
        h, w, d = dims
        volume = h * w * d
        try:
            items.append(
                ItemRecord(
                    item_id=str(row["item_id"]),
                    demand_frequency=float(row["demand_frequency"]),
                    height=h,
                    width=w,
                    depth=d,
                    volume=volume,
                    current_stock=int(row["current_stock"]),
                    weight_per_unit=float(row["weight_per_unit"]),
                )
            )
        except Exception as e:  # noqa: BLE001
            LOGGER.error("Skipping item at row %s due to error: %s", idx, e)
    # Sort high demand -> low
    items.sort(key=lambda x: (-x.demand_frequency, x.item_id))
    return items


def is_packing_row(row: pd.Series) -> bool:
    # Various heuristics to detect packing station rows
    lt = str(row.get("location_type", "")).lower()
    lid = str(row.get("location_id", "")).lower()
    is_flag = str(row.get("is_packing_station", "")).lower() in {"1", "true", "yes"}
    x = row.get("x_coord", None)
    y = row.get("y_coord", None)
    coord_is_origin = False
    try:
        coord_is_origin = float(x) == 0 and float(y) == 0
    except Exception:  # noqa: BLE001
        pass
    return ("packing" in lt) or ("packing" in lid) or is_flag or coord_is_origin


def load_locations(path: str) -> List[LocationRecord]:
    df = pd.read_csv(path)
    required = {"location_id", "x_coord", "y_coord", "max_size", "max_weight"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Missing required location columns: {missing}")

    # Filter out packing station rows
    filtered = df[~df.apply(is_packing_row, axis=1)].copy()

    locations: List[LocationRecord] = []
    for idx, row in filtered.iterrows():
        try:
            x = float(row["x_coord"])
            y = float(row["y_coord"])
            distance = math.sqrt(x * x + y * y)
            locations.append(
                LocationRecord(
                    location_id=str(row["location_id"]),
                    x=x,
                    y=y,
                    max_size=float(row["max_size"]),
                    max_weight=float(row["max_weight"]),
                    distance=distance,
                )
            )
        except Exception as e:  # noqa: BLE001
            LOGGER.error("Skipping location row %s due to error: %s", idx, e)
    # Sort accessible (closest) -> far
    locations.sort(key=lambda loc: (loc.distance, loc.location_id))
    return locations


def greedy_assign(items: List[ItemRecord], locations: List[LocationRecord]) -> pd.DataFrame:
    placement_rows = []
    for item in items:
        assigned_location = None
        # Skip early if obviously invalid volume
        if item.volume <= 0:
            placement_rows.append({"item_id": item.item_id, "recommended_location": "UNPLACED"})
            continue
        for loc in locations:
            if loc.occupied:
                continue
            if item.volume <= loc.max_size and item.total_weight <= loc.max_weight:
                assigned_location = loc.location_id
                loc.occupied = True
                break
        if not assigned_location:
            assigned_location = "UNPLACED"
        placement_rows.append({"item_id": item.item_id, "recommended_location": assigned_location})
    return pd.DataFrame(placement_rows)


def run(items_path: str, layout_path: str, output_path: str) -> str:
    LOGGER.info("Loading items from %s", items_path)
    items = load_items(items_path)
    LOGGER.info("Loaded %d items", len(items))
    LOGGER.info("Loading locations from %s", layout_path)
    locations = load_locations(layout_path)
    LOGGER.info("Loaded %d candidate locations", len(locations))

    if not items:
        raise RuntimeError("No valid items loaded. Aborting.")
    if not locations:
        LOGGER.warning("No locations available. All items will be UNPLACED.")

    LOGGER.info("Running greedy assignment...")
    df_result = greedy_assign(items, locations)
    df_result.to_csv(output_path, index=False)
    LOGGER.info("Placement recommendations written to %s", output_path)
    return output_path


def configure_logging(verbose: bool):
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s | %(levelname)-8s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def parse_args(argv: List[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Inventory Placement Optimization (Greedy)")
    parser.add_argument("--items", default="item_attributes.csv", help="Path to item attributes CSV")
    parser.add_argument("--layout", default="warehouse_layout.csv", help="Path to warehouse layout CSV")
    parser.add_argument("--output", default="placement_recommendations.csv", help="Output CSV path")
    parser.add_argument("--verbose", action="store_true", help="Enable debug logging")
    return parser.parse_args(argv)


def main(argv: Optional[List[str]] = None) -> int:
    args = parse_args(argv or sys.argv[1:])
    configure_logging(args.verbose)

    for p in [args.items, args.layout]:
        if not os.path.exists(p):
            LOGGER.error("Required input file not found: %s", p)
            return 1
    try:
        run(args.items, args.layout, args.output)
    except Exception as e:  # noqa: BLE001
        LOGGER.exception("Failed to generate placement recommendations: %s", e)
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
