    if max_distance <= 0:
        norm_dist_component = 1.0
    else:
        norm_dist_component = 1.0 - (distance / max_distance)
    demand_norm = demand_frequency / (demand_frequency + 10.0)
    base_reward = norm_dist_component + DEMAND_WEIGHT * demand_norm
    bonus = 0.0
    if aisle is not None and aisle == "A19" and demand_frequency > 100:
        bonus = 0.5 * (demand_frequency / 100)
    return base_reward + bonus
"""Incremental placement module combining rule-based heuristic with a lightweight
Q-learning (multi-armed bandit style) component for new incoming items.

Function: place_new_item(new_item: dict) -> str

Inputs & Files (expected in CWD):
  - placement_recommendations.csv  (existing placements; columns: item_id,recommended_location)
  - warehouse_layout.csv OR locations_data.csv (layout with columns: location_id,x_coord,y_coord,max_size,max_weight[,is_shelf])

new_item dict example:
    new_item = {
        "item_id": "ITEM123",
        "demand_frequency": 20,
        "dimensions": 5,              # Either numeric volume OR string "h x w x d"
        "current_stock": 10,
        "weight_per_unit": 2
    }

Logic Steps:
 1. Load layout & existing placements; determine available (unoccupied) shelves.
 2. Rule-based: compute Euclidean distance to packing station (0,0) and sort ascending.
 3. Feasibility: dimensions (volume) <= max_size and (current_stock * weight_per_unit) <= max_weight.
 4. RL (epsilon-greedy Q-learning) over feasible shelves:
       - State bucket: demand_bucket = floor(demand_frequency / DEMAND_BUCKET_SIZE)
       - Actions: feasible shelf IDs
       - Reward: (1 - normalized_distance) + DEMAND_WEIGHT * demand_norm
       - Q update: Q[s,b] <- (1-alpha)*Q + alpha*reward (single-step episode)
       - Train for N_EPISODES per call (lightweight, cumulative across runs via pickle file).
 5. Choose best shelf by highest Q value for the state bucket; fallback to first feasible (by distance) if tie/empty.
 6. Append placement to placement_recommendations.csv persistently.

Persistence:
  - Q table stored in 'placement_q_table.pkl' as dict: {(demand_bucket, shelf_id): q_value}

Returns:
  - Success: "New item ITEM123 placed at location L5"
  - Failure (no feasible shelves): "No suitable location found for item ITEM123"

Extensibility Notes:
  - Replace bandit with full Q-learning by modeling post-placement state if sequential decisions required.
  - Add shelf rebalancing / dynamic evaporation to Q values to prevent overfitting.
  - Incorporate congestion / aisle travel time instead of pure Euclidean distance.

Author: Auto-generated by GitHub Copilot
"""
from __future__ import annotations

import math
import os
import pickle
import json
import argparse
from typing import Dict, List, Tuple, Optional, Any

import numpy as np
import pandas as pd

Q_TABLE_PATH = "placement_q_table.pkl"
PLACEMENTS_FILE = "placement_recommendations.csv"
LAYOUT_CANDIDATE_FILENAMES = ["warehouse_layout.csv", "locations_data.csv", "locations_data_extended.csv"]  # extended file added

# RL Hyperparameters
ALPHA = 0.3             # learning rate
N_EPISODES = 40         # episodes per new item (small to stay fast)
DEMAND_BUCKET_SIZE = 25  # bucket width for demand frequency
DEMAND_WEIGHT = 0.1      # weight coefficient for demand in reward

# Epsilon (exploration) dynamic decay
EPSILON_START = 0.25
EPSILON_MIN = 0.05
EPSILON_DECAY = 0.995   # multiplicative per update step

META_PATH = "rl_meta.json"  # stores {steps, epsilon}


def _load_q_table() -> Dict[Any, float]:
    if os.path.exists(Q_TABLE_PATH):
        try:
            with open(Q_TABLE_PATH, "rb") as f:
                return pickle.load(f)
        except Exception:
            return {}
    return {}


def _load_meta() -> Dict[str, Any]:
    if os.path.exists(META_PATH):
        try:
            with open(META_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
            if "steps" not in data:
                data["steps"] = 0
            if "epsilon" not in data:
                data["epsilon"] = EPSILON_START
            return data
        except Exception:
            pass
    return {"steps": 0, "epsilon": EPSILON_START}


def _save_meta(meta: Dict[str, Any]) -> None:
    try:
        with open(META_PATH, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2)
    except Exception:
        pass


def _save_q_table(q: Dict[Tuple[int, str], float]):
    with open(Q_TABLE_PATH, "wb") as f:
        pickle.dump(q, f)


def _parse_volume(dim_value) -> Optional[float]:
    """Parse volume from input which can be:
        - numeric (int/float): interpreted directly as volume
        - string formatted like "h x w x d" (any separators x,*,X or spaces)
      Returns None if cannot parse.
    """
    if dim_value is None:
        return None
    if isinstance(dim_value, (int, float)):
        return float(dim_value)
    if isinstance(dim_value, str):
        # Extract first 3 numbers; if only 1 treat as direct volume
        parts = [p for p in dim_value.replace("*", "x").replace("X", "x").replace(" ", "x").split("x") if p]
        nums = []
        for p in parts:
            try:
                nums.append(float(p))
            except ValueError:
                continue
        if not nums:
            return None
        if len(nums) == 1:
            return nums[0]
        while len(nums) < 3:
            nums.append(1.0)  # assume missing dimension = 1
        h, w, d = nums[:3]
        return h * w * d
    return None


def _load_layout(layout_override: Optional[str] = None) -> pd.DataFrame:
    """Load a layout; if layout_override provided, use it; else search candidates."""
    candidates = [layout_override] if layout_override else LAYOUT_CANDIDATE_FILENAMES
    for fname in candidates:
        if fname and os.path.exists(fname):
            df = pd.read_csv(fname)
            required = {"location_id", "x_coord", "y_coord", "max_size", "max_weight"}
            missing = required - set(df.columns)
            if missing:
                raise ValueError(f"Layout file {fname} is missing required columns: {missing}")
            return df
    raise FileNotFoundError("No warehouse layout file found (checked: " + ", ".join(candidates) + ")")


def _load_existing_placements() -> pd.DataFrame:
    if os.path.exists(PLACEMENTS_FILE):
        df = pd.read_csv(PLACEMENTS_FILE)
        # Ensure extended columns exist
        for col in ["allocated_volume", "allocated_weight", "remaining_size", "remaining_weight"]:
            if col not in df.columns:
                df[col] = None
        # If file originally only had two columns, we keep rows but treat them as historical placements without capacity deduction
        # Future placements will populate new columns; residuals derived only from rows with allocated_* present.
        return df
    return pd.DataFrame(columns=["item_id", "recommended_location", "allocated_volume", "allocated_weight", "remaining_size", "remaining_weight"])


def _get_available_shelves(layout_df: pd.DataFrame, placements_df: pd.DataFrame) -> pd.DataFrame:
    # Start with all shelves (filter packing, etc.)
    df = layout_df.copy()
    if "is_shelf" in df.columns:
        df = df[df["is_shelf"] == True]  # noqa: E712
    df = df[~df["location_id"].str.lower().str.contains("packing")]
    df = df[(df["x_coord"] != 0) | (df["y_coord"] != 0)]

    # Aggregate existing allocations to compute residuals
    if not placements_df.empty:
        agg = placements_df[placements_df["recommended_location"].notna() & (placements_df["recommended_location"] != "UNPLACED")]
        if not agg.empty:
            grouped = agg.groupby("recommended_location").agg({
                "allocated_volume": "sum",
                "allocated_weight": "sum"
            }).rename(columns={"allocated_volume": "used_volume", "allocated_weight": "used_weight"})
            df = df.merge(grouped, left_on="location_id", right_index=True, how="left")
            # Avoid chained assignment FutureWarning by assigning the result of fillna
            df["used_volume"] = df["used_volume"].fillna(0.0)
            df["used_weight"] = df["used_weight"].fillna(0.0)
            df["remaining_size"] = (df["max_size"] - df["used_volume"]).clip(lower=0)
            df["remaining_weight"] = (df["max_weight"] - df["used_weight"]).clip(lower=0)
        else:
            df["remaining_size"] = df["max_size"]
            df["remaining_weight"] = df["max_weight"]
    else:
        df["remaining_size"] = df["max_size"]
        df["remaining_weight"] = df["max_weight"]
    return df


def _feasible_shelves(available_df: pd.DataFrame, volume: float, total_weight: float) -> pd.DataFrame:
    # Use remaining capacity columns if present, else full capacity
    size_col = "remaining_size" if "remaining_size" in available_df.columns else "max_size"
    weight_col = "remaining_weight" if "remaining_weight" in available_df.columns else "max_weight"
    return available_df[(volume <= available_df[size_col]) & (total_weight <= available_df[weight_col])]


def _compute_distances(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # For new priority: higher aisle (x_coord) is 'closer'.
    # Assign 'priority' as x_coord (higher is better), then y_coord (higher is better)
    df["priority"] = df["x_coord"] * 1000 + df["y_coord"]
    # Still compute Euclidean distance for RL reward
    df["distance"] = np.sqrt(df["x_coord"] ** 2 + df["y_coord"] ** 2)
    return df


def _demand_bucket(demand: float) -> int:
    return int(demand // DEMAND_BUCKET_SIZE)


def _epsilon_greedy_select(q_table: Dict[Any, float], state_key_partial: Tuple[int, int, int], shelves: List[str], epsilon: float) -> str:
    if not shelves:
        raise ValueError("No shelves provided to epsilon-greedy selection")
    if np.random.rand() < epsilon:
        return np.random.choice(shelves)
    # Exploit: pick shelf with max Q
    best_shelf = None
    best_q = -float("inf")
    for sid in shelves:
        full_key = (*state_key_partial, sid)
        # Backward compatibility: if not present, fall back to legacy (demand_bucket, sid)
        legacy_key = (state_key_partial[0], sid)
        q = q_table.get(full_key, q_table.get(legacy_key, 0.0))
        if q > best_q:
            best_q = q
            best_shelf = sid
    return best_shelf if best_shelf is not None else np.random.choice(shelves)


def _reward(distance: float, max_distance: float, demand_frequency: float, *, volume: float, shelf_capacity: float, remaining_before: float) -> float:
    """Composite reward with distance, demand, and fit shaping.

    Distance term: 1 - (d/max_d)
    Demand term: demand / (demand + 10)
    Fit shaping (encourages avoiding tiny leftover space or excessive leftover):
        residual_after = remaining_before - volume
        residual_ratio_after = residual_after / shelf_capacity
        Target residual band: [0.25, 0.60] (score=1 inside band)
        Penalize below 0.25 (fragmentation) and above 0.60 (under-use)
    Weighted combination: distance + DEMAND_WEIGHT * demand + FIT_WEIGHT * fit
    """
    if max_distance <= 0:
        norm_dist_component = 1.0
    else:
        norm_dist_component = 1.0 - (distance / max_distance)
    demand_norm = demand_frequency / (demand_frequency + 10.0)
    FIT_WEIGHT = 0.15
    if shelf_capacity <= 0:
        fit_score = 0.0
    else:
        residual_after = max(0.0, remaining_before - volume)
        residual_ratio_after = residual_after / shelf_capacity
        if 0.25 <= residual_ratio_after <= 0.60:
            fit_score = 1.0
        elif residual_ratio_after < 0.25:
            fit_score = max(0.0, (residual_ratio_after / 0.25) * 0.6)
        else:  # >0.60
            over_span = min(1.0, residual_ratio_after)
            fit_score = max(0.0, 1 - (over_span - 0.60) / 0.40)
    return norm_dist_component + DEMAND_WEIGHT * demand_norm + FIT_WEIGHT * fit_score


def place_new_item(new_item: dict) -> str:
    """Place a new item using hybrid rule-based + Q-learning (bandit) approach.

    Side-effect: appends placement to placement_recommendations.csv and updates Q table.
    """
    return _place_new_item_core(new_item)


def _place_new_item_core(new_item: dict, layout_override: Optional[str] = None) -> str:
    # Optimistic Q-value initialization for A19 shelves
    for sid in shelves:
        aisle = sid.split("-")[0] if "-" in sid else ""
        if aisle == "A19" and (bucket, sid) not in q_table:
            q_table[(bucket, sid)] = 1.0  # optimistic initial Q-value
    for _ in range(N_EPISODES):
        action_shelf = _epsilon_greedy_select(q_table, bucket, shelves, EPSILON)
        dist = float(feasible_df.loc[feasible_df.location_id == action_shelf, "distance"].iloc[0])
        aisle = action_shelf.split("-")[0] if "-" in action_shelf else ""
        r = _reward(dist, max_distance, demand_freq, aisle)
        key = (bucket, action_shelf)
        old_q = q_table.get(key, 0.0)
        new_q = (1 - ALPHA) * old_q + ALPHA * r
        q_table[key] = new_q
    """Internal core with optional layout override (batch support)."""
    required_keys = {"item_id", "demand_frequency", "dimensions", "current_stock", "weight_per_unit"}
    missing = required_keys - set(new_item.keys())
    if missing:
        raise ValueError(f"new_item dict missing required keys: {missing}")

    # Parse volume
    volume = _parse_volume(new_item.get("dimensions"))
    if volume is None or volume <= 0:
        return f"No suitable location found for item {new_item['item_id']}"  # cannot place with invalid volume

    total_weight = float(new_item["current_stock"]) * float(new_item["weight_per_unit"])
    demand_freq = float(new_item["demand_frequency"])

    # Load data
    layout_df = _load_layout(layout_override)
    placements_df = _load_existing_placements()
    available_df = _get_available_shelves(layout_df, placements_df)
    if available_df.empty:
        return f"No suitable location found for item {new_item['item_id']}"

    # Feasible subset
    feasible_df = _feasible_shelves(available_df, volume, total_weight)
    if feasible_df.empty:
        return f"No suitable location found for item {new_item['item_id']}"

    # Compute priority & sort for rule-based baseline
    feasible_df = _compute_distances(feasible_df)
    feasible_df = feasible_df.sort_values(by=["distance", "location_id"]).copy()

    # RL training (bandit per state bucket)
    q_table = _load_q_table()
    meta = _load_meta()
    current_epsilon = float(meta.get("epsilon", EPSILON_START))
    steps = int(meta.get("steps", 0))
    demand_bucket = _demand_bucket(demand_freq)
    # Space bucket (pre-placement remaining ratio coarse buckets 0-4)
    def space_bucket(rem_ratio: float) -> int:
        if rem_ratio >= 0.8: return 4
        if rem_ratio >= 0.6: return 3
        if rem_ratio >= 0.4: return 2
        if rem_ratio >= 0.2: return 1
        return 0
    # Diversity bucket: count of prior distinct items already on shelf (from placements_df)
    diversity_map = {}
    if not placements_df.empty:
        placed_hist = placements_df[placements_df["recommended_location"].notna() & (placements_df["recommended_location"] != "UNPLACED")]
        if not placed_hist.empty:
            diversity_map = placed_hist.groupby("recommended_location")["item_id"].nunique().to_dict()
    shelves = feasible_df["location_id"].tolist()
    max_distance = feasible_df["distance"].max()

    # Optimistic Q-value initialization for A19 shelves
    for sid in shelves:
        aisle = sid.split("-")[0] if "-" in sid else ""
        if aisle == "A19" and (bucket, sid) not in q_table:
            q_table[(bucket, sid)] = 1.0  # optimistic initial Q-value
    for _ in range(N_EPISODES):
        # Use mean remaining ratio across feasible shelves just for bucket derivation per action
        # Epsilon-greedy uses a partial state (demand_bucket, generic space bucket  ) -> we approximate with average remaining ratio of chosen shelf each iteration
        # We compute state per chosen shelf after selection for update.
        # For selection, we approximate using median remaining ratio to build a stable partial key.
        median_remaining_ratio = 0.0
        if "remaining_size" in feasible_df.columns:
            cap_series = feasible_df.apply(lambda r: float(r.get("remaining_size", r.get("max_size", 0.0))) / float(r.get("max_size", 1.0)), axis=1)
            if not cap_series.empty:
                median_remaining_ratio = float(cap_series.median())
        space_bucket_partial = space_bucket(median_remaining_ratio)
        diversity_bucket_partial = 0  # partial key diversity ignored for selection; refined per shelf update
        state_key_partial = (demand_bucket, space_bucket_partial, diversity_bucket_partial)
        action_shelf = _epsilon_greedy_select(q_table, state_key_partial, shelves, current_epsilon)
        row_sel = feasible_df.loc[feasible_df.location_id == action_shelf].iloc[0]
        dist = float(row_sel["distance"])
        shelf_capacity = float(row_sel.get("max_size", 0.0))
        remaining_before = float(row_sel.get("remaining_size", shelf_capacity))
        rem_ratio = remaining_before / shelf_capacity if shelf_capacity > 0 else 0.0
        sp_bucket = space_bucket(rem_ratio)
        div_count = diversity_map.get(action_shelf, 0)
        if div_count >= 8:
            diversity_bucket = 4
        elif div_count >= 5:
            diversity_bucket = 3
        elif div_count >= 3:
            diversity_bucket = 2
        elif div_count >= 1:
            diversity_bucket = 1
        else:
            diversity_bucket = 0
        r = _reward(dist, max_distance, demand_freq, volume=volume, shelf_capacity=shelf_capacity, remaining_before=remaining_before)
        full_key = (demand_bucket, sp_bucket, diversity_bucket, action_shelf)
        legacy_key = (demand_bucket, action_shelf)
        old_q = q_table.get(full_key, q_table.get(legacy_key, 0.0))
        new_q = (1 - ALPHA) * old_q + ALPHA * r
        q_table[full_key] = new_q
        # Update exploration schedule
        steps += 1
        current_epsilon = max(EPSILON_MIN, current_epsilon * EPSILON_DECAY)

    # Choose best shelf based on learned Q values
    best_shelf = None
    best_q = -float("inf")
    for sid in shelves:
        # Evaluate best across enriched state variants; fall back to legacy if none
        candidate_qs = [v for k, v in q_table.items() if isinstance(k, tuple) and len(k) == 4 and k[-1] == sid and k[0] == demand_bucket]
        legacy_q = q_table.get((demand_bucket, sid), None)
        if not candidate_qs and legacy_q is not None:
            candidate_qs = [legacy_q]
        q_val = max(candidate_qs) if candidate_qs else 0.0
        if q_val > best_q:
            best_q = q_val
            best_shelf = sid

    # Fallback if for some reason best_shelf is None
    if best_shelf is None:
        best_shelf = shelves[0]  # first feasible by distance

    # Persist placement
    # Update residuals after selecting shelf
    # Recompute residual fields for chosen shelf (if dataset supports them)
    allocated_volume = volume
    allocated_weight = total_weight
    remaining_size = None
    remaining_weight = None
    if "remaining_size" in feasible_df.columns:
        row_sel = feasible_df[feasible_df.location_id == best_shelf].iloc[0]
        size_col = "remaining_size" if "remaining_size" in row_sel else "max_size"
        weight_col = "remaining_weight" if "remaining_weight" in row_sel else "max_weight"
        remaining_size = float(row_sel[size_col]) - allocated_volume
        remaining_weight = float(row_sel[weight_col]) - allocated_weight
    new_row = {
        "item_id": new_item["item_id"],
        "recommended_location": best_shelf,
        "allocated_volume": allocated_volume,
        "allocated_weight": allocated_weight,
        "remaining_size": remaining_size,
        "remaining_weight": remaining_weight,
    }
    placements_df = pd.concat([placements_df, pd.DataFrame([new_row])], ignore_index=True)
    placements_df.to_csv(PLACEMENTS_FILE, index=False)

    # Persist Q table
    _save_q_table(q_table)
    meta["steps"] = steps
    meta["epsilon"] = current_epsilon
    _save_meta(meta)

    # Append epsilon info if meta present for transparency during CLI usage
    eps_info = ""
    if os.path.exists(META_PATH):
        try:
            with open(META_PATH, "r", encoding="utf-8") as f:
                m = json.load(f)
            eps_info = f" (epsilon={m.get('epsilon'):.4f}, steps={m.get('steps')})"
        except Exception:
            pass
    return f"New item {new_item['item_id']} placed at location {best_shelf}{eps_info}"


def _interactive_prompt() -> dict:
    print("Enter new item details (press Ctrl+C to cancel):")
    def _ask(prompt: str, cast, allow_empty=False):
        while True:
            val = input(prompt).strip()
            if not val and allow_empty:
                return None
            try:
                return cast(val) if cast else val
            except Exception:
                print("Invalid value, try again.")

    item_id = _ask("Item ID: ", str)
    demand = _ask("Demand frequency (number): ", float)
    dims = _ask("Dimensions (volume or HxWxD like 0.2x0.3x0.4): ", str)
    stock = _ask("Current stock (int): ", int)
    wpu = _ask("Weight per unit (number, default 1) : ", float)

    return {
        "item_id": item_id,
        "demand_frequency": demand,
        "dimensions": dims,
        "current_stock": stock,
        "weight_per_unit": wpu,
    }


def _demo():  # pragma: no cover
    sample_item = {"item_id": "ITEM_DEMO_001", "demand_frequency": 37, "dimensions": "0.3x0.3x0.4", "current_stock": 25, "weight_per_unit": 0.8}
    print(place_new_item(sample_item))


def main():  # pragma: no cover - CLI entry
    parser = argparse.ArgumentParser(description="Incremental RL-based placement")
    parser.add_argument("--demo", action="store_true", help="Run demo placement and exit")
    parser.add_argument("--interactive", action="store_true", help="Run interactive prompt for a single new item (default if no mode provided)")
    parser.add_argument("--batch", metavar="CSV", help="Process a CSV of new orders (columns: item_id,demand_frequency,dimensions,current_stock,weight_per_unit)")
    parser.add_argument("--layout", metavar="LAYOUT_CSV", help="Explicit layout file to use (overrides auto-detect)")
    parser.add_argument("--episodes", type=int, help="Override training episodes per item (default 40)")
    args = parser.parse_args()

    global N_EPISODES
    if args.episodes:
        N_EPISODES = args.episodes

    if args.demo:
        _demo()
        return 0

    if args.batch:
        if not os.path.exists(args.batch):
            print(f"[ERROR] Batch file not found: {args.batch}")
            return 1
        df_batch = pd.read_csv(args.batch)
        required_cols = {"item_id", "demand_frequency", "dimensions", "current_stock", "weight_per_unit"}
        missing = required_cols - set(df_batch.columns)
        if missing:
            print(f"[ERROR] Batch file missing columns: {missing}")
            return 1
        results = []
        for _, row in df_batch.iterrows():
            new_item = {
                "item_id": row["item_id"],
                "demand_frequency": row["demand_frequency"],
                "dimensions": row["dimensions"],
                "current_stock": row["current_stock"],
                "weight_per_unit": row["weight_per_unit"],
            }
            res = _place_new_item_core(new_item, layout_override=args.layout)
            results.append(res)
            print(res)
        print(f"Processed {len(results)} new orders.")
        return 0

    # Default interactive
    try:
        item = _interactive_prompt()
        result = _place_new_item_core(item, layout_override=args.layout)
        print("\n" + result)
    except KeyboardInterrupt:
        print("\nCancelled.")
        return 1
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
