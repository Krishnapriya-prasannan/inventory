"""Data preparation utilities for warehouse optimization experiments.

Features:
 1. Generate training/testing datasets from historical placements
    - Joins item, location, and placement data
    - Computes engineered features
    - Labels quality of placement (distance rank & optimal flags)
 2. Extend the locations layout with additional aisles to increase capacity

Outputs:
  - training_data.csv
  - testing_data.csv
  - locations_data_extended.csv (or optionally in-place update if --inplace specified)

Usage examples:
  python data_preparation.py --split               # only build train/test
  python data_preparation.py --extend --new-aisles 3  # only extend locations by 3 aisles
  python data_preparation.py --all                  # do both (default)

Author: Auto-generated by GitHub Copilot
"""
from __future__ import annotations

import argparse
import json
import os
import random
from dataclasses import dataclass
from typing import Optional, Tuple

import numpy as np
import pandas as pd

PLACEMENTS_FILE = "placement_recommendations.csv"
LOCATIONS_FILE = "locations_data.csv"  # existing
INVENTORY_FILE = "inventory_data.csv"
EXTENDED_LOCATIONS_FILE = "locations_data_extended.csv"


@dataclass
class ParsedDimensions:
    height: float
    width: float
    depth: float

    @property
    def volume(self) -> float:
        return self.height * self.width * self.depth


def parse_dimensions(dim_str: str) -> Optional[ParsedDimensions]:
    if not isinstance(dim_str, str):
        return None
    s = dim_str.strip()
    # Try JSON
    try:
        if s.startswith("{"):
            j = json.loads(s)
            return ParsedDimensions(float(j["height"]), float(j["width"]), float(j["depth"]))
    except Exception:  # noqa: BLE001
        pass
    # Try simple split (e.g. 0.2x0.3x0.4)
    for sep in ["x", "*", "X", " "]:
        if sep in s:
            parts = [p for p in s.replace("*", "x").replace("X", "x").replace(" ", "x").split("x") if p]
            nums = []
            for p in parts:
                try:
                    nums.append(float(p))
                except ValueError:
                    pass
            if len(nums) == 1:
                # treat single number as already volume; cube root distribution assumption
                v = nums[0]
                c = v ** (1 / 3)
                return ParsedDimensions(c, c, c)
            if len(nums) >= 3:
                h, w, d = nums[:3]
                return ParsedDimensions(h, w, d)
    return None


def build_training_test_sets(
    placements_path: str = PLACEMENTS_FILE,
    locations_path: str = LOCATIONS_FILE,
    inventory_path: str = INVENTORY_FILE,
    train_out: str = "training_data.csv",
    test_out: str = "testing_data.csv",
    test_size: float = 0.2,
    seed: int = 42,
) -> Tuple[str, str]:
    if not (os.path.exists(placements_path) and os.path.exists(locations_path) and os.path.exists(inventory_path)):
        raise FileNotFoundError("One or more required input files are missing.")

    placements = pd.read_csv(placements_path)
    locations = pd.read_csv(locations_path)
    inventory = pd.read_csv(inventory_path)

    # Drop unplaced items
    placements = placements[placements["recommended_location"].notna() & (placements["recommended_location"] != "UNPLACED")].copy()
    if placements.empty:
        raise ValueError("No valid placement records to build datasets.")

    # Parse item volumes & total weight
    inventory["parsed_volume"] = inventory["dimensions"].apply(lambda x: (parse_dimensions(x).volume if parse_dimensions(x) else None))
    inventory["total_weight"] = inventory["current_stock"] * 1.0  # weight_per_unit may be missing; assume 1
    if "weight_per_unit" in inventory.columns:
        inventory["total_weight"] = inventory["current_stock"] * inventory["weight_per_unit"].fillna(1.0)

    # Compute distance for all locations
    locations["distance"] = np.sqrt(locations["x_coord"] ** 2 + locations["y_coord"] ** 2)

    df = placements.merge(inventory[["item_id", "demand_frequency", "parsed_volume", "total_weight"]], on="item_id", how="left") \
        .merge(locations[["location_id", "max_size", "max_weight", "distance"]], left_on="recommended_location", right_on="location_id", how="left")

    # Feature engineering
    df["volume_utilization"] = df["parsed_volume"] / df["max_size"]
    df["weight_utilization"] = df["total_weight"] / df["max_weight"]
    # Demand bucket with NaN-safe handling
    df["demand_bucket"] = (df["demand_frequency"] // 25).where(df["demand_frequency"].notna())
    df["demand_bucket"] = df["demand_bucket"].astype('Int64')  # nullable integer

    # For each item compute distance rank of chosen location among all feasible shelves
    feasible_stats = []
    loc_cap = locations[["location_id", "max_size", "max_weight", "distance"]]
    for _, row in df.iterrows():
        if pd.isna(row["parsed_volume"]) or pd.isna(row["total_weight"]):
            feasible_stats.append((None, None, None))
            continue
        feas = loc_cap[(row["parsed_volume"] <= loc_cap["max_size"]) & (row["total_weight"] <= loc_cap["max_weight"])]
        if feas.empty:
            feasible_stats.append((None, None, None))
            continue
        feas = feas.sort_values("distance")
        distances = feas["distance"].tolist()
        chosen_distance = row["distance"]
        try:
            rank = distances.index(chosen_distance) + 1  # 1-based
        except ValueError:
            rank = None
        pct = rank / len(distances) if rank else None
        top_quartile = int(pct is not None and pct <= 0.25)
        feasible_stats.append((rank, pct, top_quartile))
    df[["distance_rank", "distance_rank_pct", "is_top_quartile"]] = pd.DataFrame(feasible_stats, index=df.index)

    # Clean inf / out-of-range
    for col in ["volume_utilization", "weight_utilization"]:
        df.loc[~np.isfinite(df[col]), col] = None

    # Drop rows missing critical fields
    df_model = df.dropna(subset=["parsed_volume", "total_weight", "distance", "max_size", "max_weight", "distance_rank_pct"]).copy()
    # Fill remaining nullable demand bucket with -1 for modeling
    df_model['demand_bucket'] = df_model['demand_bucket'].fillna(-1).astype(int)
    if df_model.empty:
        raise ValueError("No rows left after cleaning to form training/testing sets.")

    # Random split
    rng = np.random.default_rng(seed)
    indices = np.arange(len(df_model))
    rng.shuffle(indices)
    test_count = max(1, int(len(indices) * test_size))
    test_idx = set(indices[:test_count])
    df_model.reset_index(drop=True, inplace=True)
    train_df = df_model.loc[~df_model.index.isin(test_idx)].copy()
    test_df = df_model.loc[df_model.index.isin(test_idx)].copy()

    train_df.to_csv(train_out, index=False)
    test_df.to_csv(test_out, index=False)
    return train_out, test_out


def extend_locations(
    base_layout: str = LOCATIONS_FILE,
    output_layout: str = EXTENDED_LOCATIONS_FILE,
    new_aisles: int = 3,
    shelves_per_aisle: int = 20,
    seed: int = 123,
    inplace: bool = False,
) -> str:
    if not os.path.exists(base_layout):
        raise FileNotFoundError(f"Base layout file not found: {base_layout}")
    df = pd.read_csv(base_layout)

    # Determine existing aisle x positions (exclude packing row at 0)
    existing_x = sorted({x for x in df["x_coord"].unique() if x != 0})
    max_x = existing_x[-1] if existing_x else 0
    step = existing_x[1] - existing_x[0] if len(existing_x) >= 2 else 5

    random.seed(seed)
    rows = []
    start_aisle_index = len(existing_x) + 1
    for i in range(new_aisles):
        aisle_number = start_aisle_index + i
        x = max_x + step * (i + 1)
        for shelf in range(1, shelves_per_aisle + 1):
            location_id = f"A{aisle_number}-S{shelf}"
            max_size = round(random.uniform(0.5, 3.0), 2)
            max_weight = round(random.uniform(100, 500), 1)
            rows.append({
                "location_id": location_id,
                "x_coord": x,
                "y_coord": shelf,
                "max_size": max_size,
                "max_weight": max_weight,
                "is_shelf": True,
            })
    df_extended = pd.concat([df, pd.DataFrame(rows)], ignore_index=True)
    out_path = base_layout if inplace else output_layout
    df_extended.to_csv(out_path, index=False)
    return out_path


def main():  # pragma: no cover
    parser = argparse.ArgumentParser(description="Prepare training/testing datasets and/or extend locations.")
    parser.add_argument("--split", action="store_true", help="Create training/testing datasets")
    parser.add_argument("--extend", action="store_true", help="Extend locations layout")
    parser.add_argument("--all", action="store_true", help="Do both split and extend (default if no flag)")
    parser.add_argument("--new-aisles", type=int, default=3, help="Number of new aisles to add when extending")
    parser.add_argument("--inplace", action="store_true", help="Modify the original locations file in place")
    parser.add_argument("--test-size", type=float, default=0.2, help="Fraction for test split")
    args = parser.parse_args()

    do_split = args.split or args.all or (not args.split and not args.extend and not args.all)
    do_extend = args.extend or args.all or (not args.split and not args.extend and not args.all)

    if do_split:
        try:
            train_path, test_path = build_training_test_sets(test_size=args.test_size)
            print(f"Training/Test sets written: {train_path}, {test_path}")
        except Exception as e:  # noqa: BLE001
            print(f"[ERROR] Failed to build training/testing sets: {e}")

    if do_extend:
        try:
            out_layout = extend_locations(new_aisles=args.new_aisles, inplace=args.inplace)
            print(f"Extended locations saved to: {out_layout}")
        except Exception as e:  # noqa: BLE001
            print(f"[ERROR] Failed to extend locations: {e}")


if __name__ == "__main__":  # pragma: no cover
    main()
